{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import io\n",
    "import base64\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from lib import get_network\n",
    "from lib.agent import DQN\n",
    "from lib.logging import Logger\n",
    "from lib.rollout import ReplayBuffer\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this assignment you are going to:\n",
    "* implement DQN and doubleDQN\n",
    "* use them on CartPole, LunarLander and (Optionally) BreakOut environments\n",
    "\n",
    "### We use PyTorch for neural networks. If you are new to PyTorch see tutorials:\n",
    "https://pytorch.org/tutorials/ <br>\n",
    "https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a closer look at the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(name, seed=None, monitor=False):\n",
    "    env = gym.make(name)\n",
    "    if monitor:\n",
    "        env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "    if seed:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_video_string(infix):\n",
    "    video = io.open(\n",
    "        f'./gym-results/openaigym.video.{infix}.video000000.mp4',\n",
    "        'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    dec_str = encoded.decode('ascii')\n",
    "    src_tag = f'<source src=\"data:video/mp4;base64,{dec_str}\" type=\"video/mp4\" /></video>'\n",
    "    html = f'<video width=\"360\" height=\"auto\" alt=\"test\" controls>{src_tag}</video>'\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0xCEEEEEEB\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_root = Path(\"results\")\n",
    "log_root.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will embed a video of random policy playing CartPole-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAAD3ZtZGF0AAACrwYF//+r3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE1NSByMjkxNyAwYTg0ZDk4IC0gSC4yNjQvTVBFRy00IEFWQyBjb2RlYyAtIENvcHlsZWZ0IDIwMDMtMjAxOCAtIGh0dHA6Ly93d3cudmlkZW9sYW4ub3JnL3gyNjQuaHRtbCAtIG9wdGlvbnM6IGNhYmFjPTEgcmVmPTMgZGVibG9jaz0xOjA6MCBhbmFseXNlPTB4MzoweDExMyBtZT1oZXggc3VibWU9NyBwc3k9MSBwc3lfcmQ9MS4wMDowLjAwIG1peGVkX3JlZj0xIG1lX3JhbmdlPTE2IGNocm9tYV9tZT0xIHRyZWxsaXM9MSA4eDhkY3Q9MSBjcW09MCBkZWFkem9uZT0yMSwxMSBmYXN0X3Bza2lwPTEgY2hyb21hX3FwX29mZnNldD0tMiB0aHJlYWRzPTEyIGxvb2thaGVhZF90aHJlYWRzPTIgc2xpY2VkX3RocmVhZHM9MCBucj0wIGRlY2ltYXRlPTEgaW50ZXJsYWNlZD0wIGJsdXJheV9jb21wYXQ9MCBjb25zdHJhaW5lZF9pbnRyYT0wIGJmcmFtZXM9MyBiX3B5cmFtaWQ9MiBiX2FkYXB0PTEgYl9iaWFzPTAgZGlyZWN0PTEgd2VpZ2h0Yj0xIG9wZW5fZ29wPTAgd2VpZ2h0cD0yIGtleWludD0yNTAga2V5aW50X21pbj0yNSBzY2VuZWN1dD00MCBpbnRyYV9yZWZyZXNoPTAgcmNfbG9va2FoZWFkPTQwIHJjPWNyZiBtYnRyZWU9MSBjcmY9MjMuMCBxY29tcD0wLjYwIHFwbWluPTAgcXBtYXg9NjkgcXBzdGVwPTQgaXBfcmF0aW89MS40MCBhcT0xOjEuMDAAgAAAAhdliIQAJ//+9bF8CmrJ84oM6DIu4Zckya62IuJtAMAAJShQAAADABLJlCWzIByZd0AAABMgA4ofQZQeYiYqxUCV4g4ExQAHYATm0jrdu0S7KVJXKL1WWdzrxcEGrnQJ+WSeAXWFFxp/QNyFJ3sbGXEL1As9E641gB9vZH/JzK+KfDLvX60FacMzJ8QYC5w7AS6vI3nRm/PKyKHtXjkDs6WXcFT3zAkKipJeksdGs6oWr82H7ETRCrmufF6mL2tJ+IG5Wr6iPymRhcPnA/3gFEQAALG4KYO8i7cxcNkvEg+C023cKwANM9G721aIAEI1MC1VaKbnaTluepGhprWuLG1jT3WsKVc1SS3p57SGPSzNES1U3GnMJ4kYlDAG3Dy5dT+4ryAUkBJbwdOGSu+DMEWs60FViUdrndFY7QqTwbAKLRH1SK/4QSli5B94Wv0tUtYCSgAGbqZisrwouBEAWQd36FPT+8U4nQM5QtMKiDe/io9bS3WYEKlYsXRK2ZJYI4lNjTLviauJDT6gE5hxt195yDQZaplnf7Nyn1FfMYtkIppTa7BFYMXwt4rpFSRfbvaBSLlgYdz5+fgniQ4o5nHGCisbEI60qCGVWrO37d4BFXL3ZvSzAAADAAFNFCMPj+4+XujD/Q3QauuGMlSmog5EgZS1OAHo7Wr6mJ0QaiVcWLqnTrbfcADcAC9YAAADAAADAAADAAGjAAAAwEGaJGxCP/3hAAAEFB/zwFBCTVJ5z2ndO/1xKsCof9QWRRucIFYjb7ptk0dAjX1CldT6W2sYkbFXitSjEgWoyVOgOXInkQl4uWKPeaWYyW4RkSZ/uMUPdxP3ZNbC2GTEnoBiGxUmSfVhH+B13YKAfD7S+La6qcgSernbU02sLSa0KdC6zE4Borza7TxyNY21gD+FMy9NyGnwsTSDyTDsBTbZOP4JR2Ytqm/XcylKZ8FsNyIhJQ+PeDmh3TB7NEfJkAAAAFJBnkJ4hH8AABa2JSAI6/6KvHdy6riS0MvuzlFB1hpe0ojpDJg27ZJE/Srxah2Qb+rv+ypeARUkHYSer+Cic0hftVpeJHU5iH5DpgmgW/n7pPSpAAAAOwGeYXRH/wAAI9UhACL9W8QaM+oTnK/sUI+AR9k5ECV+EGEYQpGhjzt4JeNrIMqSYRVLe6PmvqGmWP6MAAAANwGeY2pH/wAAI8cr1Wy5mzisT9vvtKLbiPZEO1tHfm+BbrACIOhT9hRL6NYIQYfvs4aEmMOmDdEAAABWQZpoSahBaJlMCE///fEAAAMCn+3iV4A5f0Np+yp3LUGH+mqqvoMX8tkomroRHsnI7Nzul1WqqrWWz7NDgLqRBFAIZF+dM4rOK14U1+6CXwLCeXrRiI0AAAA/QZ6GRREsI/8AABa+dZ3jPheYg4zEgTR1TTDg3F1y3do2anDXrOs1kAENdM/eOZB2eBT7F0vuV0JM10Ns6cfBAAAAMwGepXRH/wAAI6v4aLlWMzOdggazAeM+MVq65WNdpXI7OMvczABO3Qp7fCKD7W/CqBBvQQAAAC8BnqdqR/8AACO/BCdht2hYkNqGIh2vGGMZPx4Otv4PCiGNj1Ccwvd7Apn7/LKDtwAAALdBmqxJqEFsmUwIT//98QAAAwKfBKURAAoNn8SPnQzd9lIzU3R7rVUBzaExoStg5TbSgXBuqiJxxOdObuuZ+QpfjpP0AltIruQnFVQyuyzv+QktUrQw1nBPG61q+yAYPB/bRlYS1TwmxtfKa0smSkyfU/5l4qkJGrkqqpIH9s8mIS0bAbR7hXM7dB4OuK5filrAbS1oq7WwZG5vRnZNuGSTY39NjAzqsvdlCrTQU+0k8YzcczmnzUAAAAA1QZ7KRRUsI/8AABa1N6UQdq7a8c9XEkSTUGhL7rZYZzSRMmIO6+AarA/sBlKIAgaokOqQ8GEAAAAkAZ7pdEf/AAAjqeqDCzwdsssV8FK362Va2K8lT1jkTGXqTr5cAAAAKAGe62pH/wAAIr8dNEzqDBJTQXMnDwem7tmkUUtD0cELITgz1LOAkmAAAACOQZrwSahBbJlMCE///fEAAAMCn+ylSobgE1xTn4bgtHrL67rAr3/7biDwsZSlGbNzx+jKmM0fz3H7iAb5N2RcXxRBZjhvOhvRteRKIKdsPSw3COTuIyrfX8YkNZBBHBWDofnTULsAXn3Cocgh715o3OpVY+iOz+9gW36AWR60dF6dY37kKtmFCf4io0DpQQAAAEFBnw5FFSwj/wAAFrNIKeC9K38pIRI9gGJ+ABMrxPhoF+vFRVmq0PewxX6LXUh02vZPufA68yy2/8EuBY15AyDDtwAAADQBny10R/8AACOr+FwogBv+Kw1C0hxwiha2nOSCXc0ewN1BgAht5uEDzB9JvJm0BKLodlsxAAAAPAGfL2pH/wAAI3qZ2PoAOe1v5oPML1AvL/IB+MdnVc96FrUPbC6xP2TbkjjgRsG8odHLp8Hko6yo0ngJsAAAAHpBmzRJqEFsmUwIR//94QAABBUWokSAOWIrA8sjiYIxHpnl7F/UUftQjVDtrHTycL1Lk3jHeqkqqVRHiOGXlPAq/q9dO5UxukNj+Mo2TWoLb3OLd5F0OZAVnW5pyy2zYS0SJ+3+QYRaqvVtqZen6tz/ADenv/NbMsT+oQAAADdBn1JFFSwj/wAAFrxDzGxhyAHTilpNgkTX17fKFs+viabNLDHrowXWmLZOh06FwPGUXPYzPhW3AAAAKQGfcXRH/wAADX9cVUuWDDkXs10fri6OtctgiJmCW68wpPYohuoqSnqgAAAANAGfc2pH/wAAI4RQNcKPXHNHNMcAkYWrJdxT8TUn7zkr6wppjWv5jy/3qJdl32A+zb/CWDAAAACuQZt3SahBbJlMCE///fEAAAMCn7rtO8AF9ipSWPjAqd/RA11UwxLCgc8JkQAbyB0yTf0qh8v28jIwXxZvP9iwj41/Yejr7H/GTUq0zcOh2EEA/6A2VWOUhYkoJdEkf+2FgNZq0qtym0Ru3k9q6a9FLR/6xae75qY8eUk0LNizrFBTTD9RZrEkVYv3YDfB6pgA0ArZ/9JiIXuDgKsyXgoMIUB3HWmVtx32AL8UXFKBAAAANkGflUUVLCP/AAAWb5bfhRDncsmLUmBKmHu4VUGnGxIO3v1EmPOjbqj1SK7+BSeWa0bMHYzDtwAAAC4Bn7ZqR/8AACO3OkJbd5wFvBg1ZSUMH3EL+DgYCY5mYuUELtqi1FbDx2IdS/8HAAAA2UGbu0moQWyZTAhH//3hAAAEFLKq1MWUIAFgk+VlPKuj9WiNYLZvFGKb6pwbRNm1SSCM/CJRgZ4q1Gy+9juima5DaAus0WNuL+r8JlwkkgJoRxvWJzlcxx1wyjPvA06AAkc85QHgpj8NL1ekDmOnP74q1PY/wyFaTlnyUKmYbz3syrZWOStih2tb+GN6I0NhoBTW93jcKlK5U/Wow4gG4wpx/Vw33CBUjFz5NwH2g2/2RMf7dS149LJqELzXUrU0O5SO4CmaRDClv1Jd4Ww9sKzTYbGu2iTkDhUAAABeQZ/ZRRUsI/8AABazK/vzc4VNwHNa9oW7DkUG7v2ksINuKhJ+TTNvpa7MYzcB3X7pORFDQlpzHMdOO+Ci2LHchRXQD8CTszve4AoBgLCaYSbGOVrU9I9YKe+HR3pDZgAAAFMBn/h0R/8AACPVIQAi+eRPu/d9ubmpTYX1LwbV1VDUvaVUnlYBUsuLSD9rgtnjsdbBcZkhSWxNdwsPYA2AILoN/DC4mrSIdRDP7xV9rNokySNDZwAAAEQBn/pqR/8AACOx6hRTR5hTW0XljQ7QKcWPhG98yAZ+V3JJ8caRCuR3NkLZQqETxWTxSCOLm73/Q9Qgm+99Fk2mlIacAgAAAIlBm/5JqEFsmUwI//yEAAAP2Z/IGqVLHQEp+0LYqLiuIGjNvcX8ABZCMqhIBvkc1z+2ul+QbvYdaDTKxTz+vv0ynzBgTZxx7Hw1dkSYhhBp+p4B9YSR6GgaVZ1b6IfCeUjnhQtfyJPgjv4xLqro1MLDVFPJt3HZ0XvKvS1BQK1QniIzvzqJT1ks+QAAAG1BnhxFFSwj/wAAFr5cxRicZ4r7VsurDqYOSnDpcgSurQpSKAGswUgAIgVFehg/3IpgF3hdwfqXcY8o0ixX/rF2pgUqHybDjM2G7n4A2i/a429DN/t4gUHuFPh6nq7+/kk2eXzY/fE/u4FG8j2pAAAAUgGePWpH/wAAI8cNteZAA5k1Tv1bHd+ph4PxYZ7luiTTWBXdZ8INOwFOxGYVnTmQtUpfIVWO2DK/j7mpClK2QC0LvEtks6Wm4hxPN81zZ06f11IAAAR3bW9vdgAAAGxtdmhkAAAAAAAAAAAAAAAAAAAD6AAAAmwAAQAAAQAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgAAA6F0cmFrAAAAXHRraGQAAAADAAAAAAAAAAAAAAABAAAAAAAAAmwAAAAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAABAAAAAAlgAAAGQAAAAAAAkZWR0cwAAABxlbHN0AAAAAAAAAAEAAAJsAAACAAABAAAAAAMZbWRpYQAAACBtZGhkAAAAAAAAAAAAAAAAAAAyAAAAHwBVxAAAAAAALWhkbHIAAAAAAAAAAHZpZGUAAAAAAAAAAAAAAABWaWRlb0hhbmRsZXIAAAACxG1pbmYAAAAUdm1oZAAAAAEAAAAAAAAAAAAAACRkaW5mAAAAHGRyZWYAAAAAAAAAAQAAAAx1cmwgAAAAAQAAAoRzdGJsAAAAmHN0c2QAAAAAAAAAAQAAAIhhdmMxAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAlgBkABIAAAASAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGP//AAAAMmF2Y0MBZAAf/+EAGWdkAB+s2UCYM+XhAAADAAEAAAMAZA8YMZYBAAZo6+PLIsAAAAAYc3R0cwAAAAAAAAABAAAAHwAAAQAAAAAUc3RzcwAAAAAAAAABAAAAAQAAAPhjdHRzAAAAAAAAAB0AAAABAAACAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAEAAAAAAIAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAEAAAAAAIAAAEAAAAAHHN0c2MAAAAAAAAAAQAAAAEAAAAfAAAAAQAAAJBzdHN6AAAAAAAAAAAAAAAfAAAEzgAAAMQAAABWAAAAPwAAADsAAABaAAAAQwAAADcAAAAzAAAAuwAAADkAAAAoAAAALAAAAJIAAABFAAAAOAAAAEAAAAB+AAAAOwAAAC0AAAA4AAAAsgAAADoAAAAyAAAA3QAAAGIAAABXAAAASAAAAI0AAABxAAAAVgAAABRzdGNvAAAAAAAAAAEAAAAwAAAAYnVkdGEAAABabWV0YQAAAAAAAAAhaGRscgAAAAAAAAAAbWRpcmFwcGwAAAAAAAAAAAAAAAAtaWxzdAAAACWpdG9vAAAAHWRhdGEAAAABAAAAAExhdmY1OC4yMC4xMDA=\" type=\"video/mp4\" /></video></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = make_env(\"CartPole-v0\", monitor=True)\n",
    "env.reset()\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    if env.step(action)[2]: break\n",
    "env.close()\n",
    "\n",
    "HTML(data=get_html_video_string(env.file_infix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "Here's the definition of a function that implements training loop. We'll get through its elements soon.\n",
    "Although, note that we use a warmup techinque to fill the buffer in the beginning of a training procedure.\n",
    "\n",
    "The loop consists of:\n",
    "* Picking an action using the policy:\n",
    "$$a = \\text{argmax}_a Q(s, a)$$\n",
    "* Sending the action to environment (`step`)\n",
    "* Sampling N examples from the ReplayBuffer:\n",
    "$$ (s_i, a_i, s^\\prime_i, r_i)_{i=1,..,N} $$\n",
    "* Updating Q-network by minimizing the loss:\n",
    "$$y = r_i + \\gamma \\text{max}_a Q_{target}(s^{\\prime}_i, a)$$\n",
    "\n",
    "$$L_i = \\text{Loss}(Q(s_i, a_i), y)$$\n",
    "$$L = \\frac{1}{N} \\sum_i L_i \\rightarrow min,$$\n",
    "where $\\text{Loss}$ is $L_2$ loss or Huber loss\n",
    "* Updating target network every `target_update_every` iteration:\n",
    "$$\\theta^\\prime \\leftarrow \\theta \\tau + \\theta^\\prime (1 - \\tau)$$\n",
    "where $\\theta^\\prime$ - paremeters of $Q_{target}$ network, <br>\n",
    "$\\theta$ - paremeters of $Q$ network,<br>\n",
    "$\\tau \\in [0, 1]$\n",
    "\n",
    "Note, that we also use a logger that writes training info to `CSV` file, `TensorBoard` and `stdout`. We use `CSV` and `stdout` throughout the notebook, but you can use TB for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 0: Implement ReplayBuffer from `lib/rollout.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    agent,\n",
    "    env,\n",
    "    *,\n",
    "    log_dir,\n",
    "    prefix,\n",
    "    buffer_size,\n",
    "    n_steps,\n",
    "    warmup_steps,\n",
    "    target_update_every,\n",
    "    log_every,\n",
    "    save_every\n",
    "):\n",
    "    \"\"\"Training loop procedure.\n",
    "    \n",
    "    Arguments:\n",
    "        agent: DQN object.\n",
    "        env: Gym environment.\n",
    "        log_dir: PathLib path to logging dir.\n",
    "        prefix: Prefix for the experiment.\n",
    "        buffer_size: Maximum capacity of the replay buffer.\n",
    "        n_steps: Total number of training iterations.\n",
    "        warmup_steps: Number of first iterations that employ random policy.\n",
    "        target_update_every: Number of iterations between target network updates.\n",
    "        log_every: Log frequency\n",
    "        save_every: Save frequency\n",
    "        \n",
    "    Returns agent after training.\n",
    "    \"\"\"\n",
    "    \n",
    "    logger = Logger(log_dir, prefix)\n",
    "    episode_reward = []\n",
    "    memory = ReplayBuffer(buffer_size)\n",
    "    \n",
    "    obs_cur = env.reset()\n",
    "    for i in range(n_steps + 1):\n",
    "        env.render()\n",
    "        if i > warmup_steps:\n",
    "            obs = torch.FloatTensor(obs_cur).unsqueeze(0)\n",
    "            act = agent.pick_action(obs)\n",
    "        else:\n",
    "            act = env.action_space.sample()\n",
    "\n",
    "        obs_prev = obs_cur\n",
    "        obs_cur, rew, done, info = env.step(act)\n",
    "        x, x_dot, theta, theta_dot = obs_cur\n",
    "        r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
    "        r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n",
    "        rew = r1 + r2\n",
    "        episode_reward.append(rew)\n",
    "\n",
    "        memory.push(obs_prev, act, obs_cur, rew, done)\n",
    "\n",
    "        if done:\n",
    "            rew = np.sum(episode_reward)\n",
    "            episode_reward = []\n",
    "            logger.log_arr_kv(\"reward/reward\", rew)\n",
    "            obs_cur = env.reset()\n",
    "            logger.reduce_arr_kv(\"reward/reward\", \"reward/reward_mean\", np.mean)\n",
    "            logger.reduce_arr_kv(\"reward/reward\", \"reward/reward_max\", np.max)\n",
    "            logger.reduce_arr_kv(\"reward/reward\", \"reward/reward_min\", np.min)\n",
    "            logger.reduce_arr_kv(\"reward/reward\", \"reward/reward_std\", np.std)\n",
    "            logger.reduce_arr_kv(\"loss/bellman_error_t\", \"loss/bellman_error\", np.mean)\n",
    "            logger.reduce_arr_kv(\"misc/q_t\", \"misc/q\", np.mean)\n",
    "            logger.reduce_arr_kv(\"misc/q_est_t\", \"misc/q_est\", np.mean)\n",
    "            logger.log_kv(\"misc/epsilon\", agent.eps)\n",
    "            logger.log_kv(\"misc/timestep\", i)\n",
    "            '''\n",
    "            logger.write_logs(skip_arrs=True)\n",
    "            '''\n",
    "\n",
    "        if i > warmup_steps:\n",
    "\n",
    "            loss, q, q_est = agent.update_value(memory)\n",
    "            logger.log_arr_kv(\"loss/bellman_error_t\", loss)\n",
    "            logger.log_arr_kv(\"misc/q_t\", q)\n",
    "            logger.log_arr_kv(\"misc/q_est_t\", q_est)\n",
    "\n",
    "            if i % target_update_every == 0:\n",
    "                agent.update_target()\n",
    "\n",
    "            if i % save_every == 0:\n",
    "                num = i // save_every + 1\n",
    "                save_dir = log_dir / \"checkpoints\"\n",
    "                save_dir.mkdir(exist_ok=True)\n",
    "                agent.save_to(save_dir, prefix=hex(num)[2:].upper())\n",
    "\n",
    "        agent.update_eps()\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "#### Task 1: Implement MLP network in `lib/network.py`\n",
    "#### Task 2: Implement DQN in `lib/agent.py`\n",
    "\n",
    "Now, run the next two cells. You should obtain the highest (500) reward in less than 100000 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note! You can try to use small `target_update_every`. E.g. value of 1 corresponds to fitted Q-iteration. The expected outcome is unstable or even divergent behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped\n",
    "\n",
    "value_network = get_network(\"mlp\")(\n",
    "    env.observation_space.shape[0],\n",
    "    64,\n",
    "    env.action_space.n)\n",
    "\n",
    "agent = DQN(\n",
    "    value_network,\n",
    "    env.action_space,\n",
    "    eps=.1,\n",
    "    eps_decay=0.,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.01,\n",
    "    discount_factor=.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-201070f39f7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtarget_update_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlog_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0msave_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-8-24aecdffe647>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(agent, env, log_dir, prefix, buffer_size, n_steps, warmup_steps, target_update_every, log_every, save_every)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mobs_cur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_steps\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_cur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mglClearColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36mclear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mThe\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mactive\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \"\"\"\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglClear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGL_COLOR_BUFFER_BIT\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGL_DEPTH_BUFFER_BIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdispatch_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/pyglet/gl/lib.py\u001b[0m in \u001b[0;36merrcheck\u001b[0;34m(result, func, arguments)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0merrcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_debug_gl_trace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = train(\n",
    "    agent,\n",
    "    env,\n",
    "    log_dir=log_root / \"cartpole-v1\",\n",
    "    prefix=\"_dqn\",\n",
    "    buffer_size=2000,\n",
    "    n_steps=10**5,\n",
    "    warmup_steps=2000,\n",
    "    target_update_every=100,\n",
    "    log_every=500,\n",
    "    save_every=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "View more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "My Youtube Channel: https://www.youtube.com/user/MorvanZhou\n",
    "More about Reinforcement learning: https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/\n",
    "Dependencies:\n",
    "torch: 0.4\n",
    "gym: 0.8.1\n",
    "numpy\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.01                   # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "TARGET_REPLACE_ITER = 100   # target update frequency\n",
    "MEMORY_CAPACITY = 2000\n",
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped\n",
    "N_ACTIONS = env.action_space.n\n",
    "N_STATES = env.observation_space.shape[0]\n",
    "ENV_A_SHAPE = 0 if isinstance(env.action_space.sample(), int) else env.action_space.sample().shape     # to confirm the shape\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(N_STATES, 50)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)   # initialization\n",
    "        self.out = nn.Linear(50, N_ACTIONS)\n",
    "        self.out.weight.data.normal_(0, 0.1)   # initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value\n",
    "\n",
    "\n",
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "\n",
    "        self.learn_step_counter = 0                                     # for target updating\n",
    "        self.memory_counter = 0                                         # for storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch transitions\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "\n",
    "        import ipdb; ipdb.set_trace()\n",
    "        \n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        q_next = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "dqn = DQN()\n",
    "\n",
    "print('\\nCollecting experience...')\n",
    "for i_episode in range(400):\n",
    "    s = env.reset()\n",
    "    ep_r = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        a = dqn.choose_action(s)\n",
    "\n",
    "        # take action\n",
    "        s_, r, done, info = env.step(a)\n",
    "\n",
    "        # modify the reward\n",
    "        x, x_dot, theta, theta_dot = s_\n",
    "        r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
    "        r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n",
    "        r = r1 + r2\n",
    "\n",
    "        dqn.store_transition(s, a, r, s_)\n",
    "\n",
    "        ep_r += r\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "            if done:\n",
    "                print('Ep: ', i_episode,\n",
    "                      '| Ep_r: ', round(ep_r, 2))\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        s = s_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at our learned policy in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"CartPole-v1\", monitor=True)\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0)\n",
    "    act = agent.pick_action(obs, force_greedy=True)\n",
    "    obs, rew, done, info = env.step(act)\n",
    "    if done: break\n",
    "env.close()\n",
    "\n",
    "HTML(data=get_html_video_string(env.file_infix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_progress(data, name=None):\n",
    "    name = name or \"Mean reward\"\n",
    "    plt.plot(data['misc/timestep'], data['reward/reward_mean'], label=name)\n",
    "    lower = np.array(data['reward/reward_mean'] - data['reward/reward_std']).clip(-1e5, +1e5)\n",
    "    upper = np.array(data['reward/reward_mean'] + data['reward/reward_std']).clip(-1e5, +1e5)\n",
    "    examples = np.array(data['misc/timestep'])\n",
    "    plt.fill_between(list(examples), list(lower), list(upper), color='blue', alpha=0.1)\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel(\"Timestep\")\n",
    "    plt.ylabel(\"Reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell plots a mean reward against training iterations. Note, that the agent may sometimes degrade its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=300)\n",
    "df = pd.read_csv(str(log_root / \"cartpole-v1\" / \"logs_dqn.csv\"), sep=\";\")\n",
    "plot_progress(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LunarLander\n",
    "\n",
    "Okay, now that we're done with very simple CartPole environment, let's move to something more complicated. \n",
    "\n",
    "LunarLander is a simulation game where a player has to control the capsule and land it on the zone marked with flags. If an agent achieves more than 200 scores, the environment is said to be solved.\n",
    "\n",
    "##### First, you should install Box2D support in Gym: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'gym[box2d]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The random policy quickly fails the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"LunarLander-v2\", monitor=True)\n",
    "env.reset()\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    if env.step(action)[2]: break\n",
    "env.close()\n",
    "\n",
    "HTML(data=get_html_video_string(env.file_infix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Play a bit with hyperparameters. You have to gain some intuition about what each of them does. You have to achieve mean score of 200 in ~200 000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"LunarLander-v2\", SEED)\n",
    "\n",
    "value_network = get_network(\"mlp\")(\n",
    "    env.observation_space.shape[0],\n",
    "    ...,\n",
    "    env.action_space.n)\n",
    "\n",
    "agent = DQN(\n",
    "    value_network,\n",
    "    env.action_space,\n",
    "    eps=...,\n",
    "    eps_decay=...,\n",
    "    batch_size=...,\n",
    "    learning_rate=...,\n",
    "    discount_factor=...,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = train(\n",
    "    agent,\n",
    "    env,\n",
    "    log_dir=log_root / \"lunarlander-v2\",\n",
    "    prefix=\"_dqn\",\n",
    "    buffer_size=...,\n",
    "    n_steps=2*10**5,\n",
    "    warmup_steps=...,\n",
    "    target_update_every=...,\n",
    "    log_every=4000,\n",
    "    save_every=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"LunarLander-v2\", monitor=True)\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0)\n",
    "    act = agent.pick_action(obs, force_greedy=True)\n",
    "    obs, rew, done, info = env.step(act)\n",
    "    if done: break\n",
    "env.close()\n",
    "\n",
    "HTML(data=get_html_video_string(env.file_infix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=300)\n",
    "df = pd.read_csv(str(log_root / \"lunarlander-v2\" / \"logs_dqn.csv\"), sep=\";\")\n",
    "plot_progress(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main drawbacks of vanila DQN is that it overestimates Q-values. More details could be found in the original paper. In a couple of words, we have to estimate an expecation of maximum, not a maximum of expectations as we do in vanila DQN. The trick is to use the second network for the updates (we use target network for this):\n",
    "\n",
    "$$y = r + \\gamma Q_{target}(s^\\prime, \\text{argmax}_a Q(s^\\prime, a) )$$\n",
    "\n",
    "Compared to \n",
    "\n",
    "$$y = r + \\gamma Q_{target}(s^\\prime, \\text{argmax}_a Q_{target}(s^\\prime, a) )$$\n",
    "\n",
    "from vanila DQN. Which is the same as:\n",
    "\n",
    "$$y = r + \\gamma \\text{max}_a Q_{target}(s^\\prime, a)$$\n",
    "\n",
    "Double Q-learning paper: https://papers.nips.cc/paper/3964-double-q-learning <br>\n",
    "Double DQN paper: https://arxiv.org/abs/1509.06461\n",
    "\n",
    "#### Task 4. Implement Double DQN in `lib/agent.py`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, let's try it for the LunarLander environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"LunarLander-v2\", SEED)\n",
    "\n",
    "value_network = get_network(\"mlp\")(\n",
    "    env.observation_space.shape[0],\n",
    "    ...,\n",
    "    env.action_space.n)\n",
    "\n",
    "agent = DQN(\n",
    "    value_network,\n",
    "    env.action_space,\n",
    "    eps=...,\n",
    "    eps_decay=...,\n",
    "    batch_size=...,\n",
    "    learning_rate=...,\n",
    "    discount_factor=...,\n",
    "    double=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = train(\n",
    "    agent,\n",
    "    env,\n",
    "    log_dir=log_root / \"lunarlander-v2\",\n",
    "    prefix=\"_double_dqn\",\n",
    "    buffer_size=...,\n",
    "    n_steps=2*10**5,\n",
    "    warmup_steps=...,\n",
    "    target_update_every=...,\n",
    "    log_every=4000,\n",
    "    save_every=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You should obtain better curve for Double DQN, however it is not always the case. For such environent the improvement could be neglectable. Although, you have to at least reach the same performance. \n",
    "### You may want to compare the performance on _CartPole_ as, unlike _LunarLander_ , its reward function is dense. However, it may be too simple to reveal any difference.\n",
    "### Thus, it is much better to compare DQN and Double DQN on Atari benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=300)\n",
    "df = pd.read_csv(str(log_root / \"lunarlander-v2\" / \"logs_dqn.csv\"), sep=\";\")\n",
    "df_double = pd.read_csv(str(log_root / \"lunarlander-v2\" / \"logs_double_dqn.csv\"), sep=\";\")\n",
    "plot_progress(df, \"DQN\")\n",
    "plot_progress(df_double, \"Double DQN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the graphs of estimated Q values by DQN and Double DQN. Could you describe what's happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=300)\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.ylabel(\"estimated Q value\")\n",
    "plt.plot(df['misc/timestep'], df['misc/q_est'], label=\"DQN\")\n",
    "plt.plot(df_double['misc/timestep'], df_double['misc/q_est'], label=\"Double DQN\")\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATARI [Optional]\n",
    "\n",
    "In this section, you can try the algorithm on the larger problem from Atari suit. This section is optional. The training procedure is computationally cumbersome, so that ideally one may want to use GPU and powerful CPU for this task. Also note, that usually people choose larger `buffer_size` for this benchmark, which may consume anormous amount of RAM. To avoid the memory overflow, you may consider to use lossless compression algorithm to save a lot of space by compressing observations. See RLLib or Catalyst.RL for example.\n",
    "\n",
    "If you decided to go through the task, note, that typical choice of hyperparameters for Atari differs from previous ones. See RLLib's configs, for example, as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'gym[atari]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clone and install `baselines` lib. We'll use atari wrappers from it.\n",
    "\n",
    "Here are commands for that:\n",
    "```bash\n",
    "git clone https://github.com/openai/baselines\n",
    "cd baselines\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
    "from lib.utils import TransposeImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(name, seed=None, monitor=False):\n",
    "    env = gym.make(name)\n",
    "\n",
    "    is_atari = hasattr(gym.envs, 'atari') and isinstance(\n",
    "        env.unwrapped, gym.envs.atari.atari_env.AtariEnv)\n",
    "\n",
    "    if is_atari:\n",
    "        max_epi_steps = 108000\n",
    "        if max_epi_steps < 0:\n",
    "            max_epi_steps = None\n",
    "\n",
    "        env = make_atari(name, max_epi_steps)\n",
    "        env = wrap_deepmind(env, True, True, True, True)\n",
    "        env = TransposeImage(env, op=[2, 0, 1])\n",
    "        \n",
    "    if monitor:\n",
    "        env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "    if seed:\n",
    "        env.seed(seed)\n",
    "\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"BreakoutNoFrameskip-v4\", monitor=True)\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    act = env.action_space.sample()\n",
    "    if env.step(act)[2]: break\n",
    "env.close()\n",
    "\n",
    "HTML(data=get_html_video_string(env.file_infix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"BreakoutNoFrameskip-v4\", SEED)\n",
    "\n",
    "value_network = get_network(\"vision\")(\n",
    "    env.observation_space.shape[0],\n",
    "    256,\n",
    "    env.action_space.n)\n",
    "\n",
    "agent = DQN(\n",
    "    value_network,\n",
    "    env.action_space,\n",
    "    eps=...,\n",
    "    eps_decay=...,\n",
    "    batch_size=...,\n",
    "    learning_rate=...,\n",
    "    discount_factor=...,\n",
    "    double=...,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = train(\n",
    "    agent,\n",
    "    env,\n",
    "    log_dir=log_root / \"breakout-v4\",\n",
    "    prefix=\"_dqn\",\n",
    "    buffer_size=...,\n",
    "    n_steps=...,\n",
    "    warmup_steps=...,\n",
    "    target_update_every=...,\n",
    "    log_every=...,\n",
    "    save_every=...,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"BreakoutNoFrameskip-v4\", monitor=True)\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0)\n",
    "    act = agent.pick_action(obs, force_greedy=True)\n",
    "    obs, rew, done, info = env.step(act)\n",
    "    if done: break\n",
    "env.close()\n",
    "\n",
    "HTML(data=get_html_video_string(env.file_infix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=300)\n",
    "df = pd.read_csv(str(log_root / \"breakout-v4\" / \"logs_dqn.csv\"), sep=\";\")\n",
    "plot_progress(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
