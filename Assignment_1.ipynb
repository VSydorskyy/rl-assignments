{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import io\n",
    "import base64\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lib import get_network\n",
    "from lib.agent import DQN\n",
    "from lib.logging import Logger, MyLogger\n",
    "from lib.rollout import ReplayBuffer\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this assignment you are going to:\n",
    "* implement DQN and doubleDQN\n",
    "* use them on CartPole, LunarLander and (Optionally) BreakOut environments\n",
    "\n",
    "### We use PyTorch for neural networks. If you are new to PyTorch see tutorials:\n",
    "https://pytorch.org/tutorials/ <br>\n",
    "https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a closer look at the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(name, seed=None, monitor=False):\n",
    "    env = gym.make(name)\n",
    "    if monitor:\n",
    "        env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "    if seed:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_video_string(infix):\n",
    "    video = io.open(\n",
    "        f'./gym-results/openaigym.video.{infix}.video000000.mp4',\n",
    "        'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    dec_str = encoded.decode('ascii')\n",
    "    src_tag = f'<source src=\"data:video/mp4;base64,{dec_str}\" type=\"video/mp4\" /></video>'\n",
    "    html = f'<video width=\"360\" height=\"auto\" alt=\"test\" controls>{src_tag}</video>'\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0xCEEEEEEB\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_root = Path(\"results\")\n",
    "log_root.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will embed a video of random policy playing CartPole-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" /></video></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = make_env(\"CartPole-v1\", monitor=True)\n",
    "env.reset()\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    if env.step(action)[2]: break\n",
    "env.close()\n",
    "\n",
    "HTML(data=get_html_video_string(env.file_infix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "Here's the definition of a function that implements training loop. We'll get through its elements soon.\n",
    "Although, note that we use a warmup techinque to fill the buffer in the beginning of a training procedure.\n",
    "\n",
    "The loop consists of:\n",
    "* Picking an action using the policy:\n",
    "$$a = \\text{argmax}_a Q(s, a)$$\n",
    "* Sending the action to environment (`step`)\n",
    "* Sampling N examples from the ReplayBuffer:\n",
    "$$ (s_i, a_i, s^\\prime_i, r_i)_{i=1,..,N} $$\n",
    "* Updating Q-network by minimizing the loss:\n",
    "$$y = r_i + \\gamma \\text{max}_a Q_{target}(s^{\\prime}_i, a)$$\n",
    "\n",
    "$$L_i = \\text{Loss}(Q(s_i, a_i), y)$$\n",
    "$$L = \\frac{1}{N} \\sum_i L_i \\rightarrow min,$$\n",
    "where $\\text{Loss}$ is $L_2$ loss or Huber loss\n",
    "* Updating target network every `target_update_every` iteration:\n",
    "$$\\theta^\\prime \\leftarrow \\theta \\tau + \\theta^\\prime (1 - \\tau)$$\n",
    "where $\\theta^\\prime$ - paremeters of $Q_{target}$ network, <br>\n",
    "$\\theta$ - paremeters of $Q$ network,<br>\n",
    "$\\tau \\in [0, 1]$\n",
    "\n",
    "Note, that we also use a logger that writes training info to `CSV` file, `TensorBoard` and `stdout`. We use `CSV` and `stdout` throughout the notebook, but you can use TB for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 0: Implement ReplayBuffer from `lib/rollout.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    agent,\n",
    "    env,\n",
    "    *,\n",
    "    buffer_size,\n",
    "    n_episods,\n",
    "    warmup_steps,\n",
    "    target_update_every,\n",
    "    do_render=False\n",
    "):\n",
    "    memory = ReplayBuffer(buffer_size)\n",
    "    \n",
    "    obs_cur = env.reset()\n",
    "    \n",
    "    episod_idx = 0\n",
    "    iteration = 0\n",
    "    livetime = 0\n",
    "    best_agent = None\n",
    "    best_score = 0\n",
    "    \n",
    "    logger = MyLogger()\n",
    "    \n",
    "    pbar = tqdm()\n",
    "    \n",
    "    while episod_idx < n_episods:\n",
    "        \n",
    "        if do_render:\n",
    "            env.render()\n",
    "        \n",
    "        if iteration > warmup_steps:\n",
    "            obs = torch.FloatTensor(obs_cur).unsqueeze(0)\n",
    "            act = agent.pick_action(obs)\n",
    "        else:\n",
    "            act = env.action_space.sample()\n",
    "\n",
    "        obs_prev = obs_cur\n",
    "        obs_cur, rew, done, info = env.step(act)\n",
    "\n",
    "        memory.push(obs_prev, act, obs_cur, rew, done)\n",
    "\n",
    "        if done:\n",
    "            obs_cur = env.reset()\n",
    "            episod_idx += 1\n",
    "            \n",
    "            if livetime > best_score:\n",
    "                best_score = livetime\n",
    "                best_agent = deepcopy(agent)\n",
    "                \n",
    "            logger.update_lt(livetime)\n",
    "            livetime = 0\n",
    "        else:\n",
    "            logger.update_lt(livetime)\n",
    "            livetime += 1\n",
    "\n",
    "        if iteration > warmup_steps:\n",
    "\n",
    "            loss, _, _ = agent.update_value(memory)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            loss = -1\n",
    "\n",
    "            if iteration % target_update_every == 0:\n",
    "                agent.update_target()\n",
    "\n",
    "        agent.update_eps()\n",
    "        iteration += 1\n",
    "        pbar.update(1)\n",
    "        logger.update_all(rew, loss, agent.eps, done)\n",
    "        \n",
    "    pbar.close()\n",
    "    logger.summarize()\n",
    "        \n",
    "    return agent, best_agent, logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "#### Task 1: Implement MLP network in `lib/network.py`\n",
    "#### Task 2: Implement DQN in `lib/agent.py`\n",
    "\n",
    "Now, run the next two cells. You should obtain the highest (500) reward in less than 100000 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note! You can try to use small `target_update_every`. E.g. value of 1 corresponds to fitted Q-iteration. The expected outcome is unstable or even divergent behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"CartPole-v1\", SEED)\n",
    "\n",
    "value_network = get_network(\"mlp\")(\n",
    "    env.observation_space.shape[0],\n",
    "    64,\n",
    "    env.action_space.n)\n",
    "\n",
    "agent = DQN(\n",
    "    value_network,\n",
    "    env.action_space,\n",
    "    eps=.2,\n",
    "    eps_decay=(.2 - .02) / 10**5,\n",
    "    eps_final=.02,\n",
    "    batch_size=32,\n",
    "    learning_rate=5e-4,\n",
    "    discount_factor=.99\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51499it [00:54, 862.44it/s] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7ef93f06626a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mn_episods\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtarget_update_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-8-f7b92d4a41e3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(agent, env, buffer_size, n_episods, warmup_steps, target_update_every, do_render)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/src/rl-assignments/lib/agent.py\u001b[0m in \u001b[0;36mupdate_value\u001b[0;34m(self, replay_buffer)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mv_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mv_s1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_s1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mv_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mv_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51517it [01:10, 862.44it/s]"
     ]
    }
   ],
   "source": [
    "agent, best_agent, history = train(\n",
    "    agent,\n",
    "    env,\n",
    "    buffer_size=5 * 10**4,\n",
    "    n_episods=10**5,\n",
    "    warmup_steps=1000,\n",
    "    target_update_every=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.livetime[history.died])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.q_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at our learned policy in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"CartPole-v1\", monitor=True)\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0)\n",
    "    act = best_agent.pick_action(obs, force_greedy=True)\n",
    "    obs, rew, done, info = env.step(act)\n",
    "    if done: break\n",
    "env.close()\n",
    "\n",
    "HTML(data=get_html_video_string(env.file_infix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LunarLander\n",
    "\n",
    "Okay, now that we're done with very simple CartPole environment, let's move to something more complicated. \n",
    "\n",
    "LunarLander is a simulation game where a player has to control the capsule and land it on the zone marked with flags. If an agent achieves more than 200 scores, the environment is said to be solved.\n",
    "\n",
    "##### First, you should install Box2D support in Gym: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'gym[box2d]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The random policy quickly fails the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"LunarLander-v2\", monitor=True)\n",
    "env.reset()\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    feedback = env.step(action)\n",
    "    if feedback[2]: break\n",
    "env.close()\n",
    "\n",
    "HTML(data=get_html_video_string(env.file_infix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Play a bit with hyperparameters. You have to gain some intuition about what each of them does. You have to achieve mean score of 200 in ~200 000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"LunarLander-v2\", SEED)\n",
    "\n",
    "value_network = get_network(\"mlp\")(\n",
    "    env.observation_space.shape[0],\n",
    "    64,\n",
    "    env.action_space.n)\n",
    "\n",
    "agent = DQN(\n",
    "    value_network,\n",
    "    env.action_space,\n",
    "    eps=.2,\n",
    "    eps_decay=(.2 - .02) / 10**5,\n",
    "    batch_size=32,\n",
    "    learning_rate=5e-4,\n",
    "    discount_factor=.99\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent, best_agent, history = train(\n",
    "    agent,\n",
    "    env,\n",
    "    buffer_size=5 * 10**4,\n",
    "    n_episods=100,\n",
    "    warmup_steps=1000,\n",
    "    target_update_every=500,\n",
    "    do_render=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"LunarLander-v2\", monitor=True)\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0)\n",
    "    act = best_agent.pick_action(obs, force_greedy=True)\n",
    "    obs, rew, done, info = env.step(act)\n",
    "    if done: break\n",
    "env.close()\n",
    "\n",
    "HTML(data=get_html_video_string(env.file_infix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main drawbacks of vanila DQN is that it overestimates Q-values. More details could be found in the original paper. In a couple of words, we have to estimate an expecation of maximum, not a maximum of expectations as we do in vanila DQN. The trick is to use the second network for the updates (we use target network for this):\n",
    "\n",
    "$$y = r + \\gamma Q_{target}(s^\\prime, \\text{argmax}_a Q(s^\\prime, a) )$$\n",
    "\n",
    "Compared to \n",
    "\n",
    "$$y = r + \\gamma Q_{target}(s^\\prime, \\text{argmax}_a Q_{target}(s^\\prime, a) )$$\n",
    "\n",
    "from vanila DQN. Which is the same as:\n",
    "\n",
    "$$y = r + \\gamma \\text{max}_a Q_{target}(s^\\prime, a)$$\n",
    "\n",
    "Double Q-learning paper: https://papers.nips.cc/paper/3964-double-q-learning <br>\n",
    "Double DQN paper: https://arxiv.org/abs/1509.06461\n",
    "\n",
    "#### Task 4. Implement Double DQN in `lib/agent.py`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, let's try it for the LunarLander environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"LunarLander-v2\", SEED)\n",
    "\n",
    "value_network = get_network(\"mlp\")(\n",
    "    env.observation_space.shape[0],\n",
    "    ...,\n",
    "    env.action_space.n)\n",
    "\n",
    "agent = DQN(\n",
    "    value_network,\n",
    "    env.action_space,\n",
    "    eps=...,\n",
    "    eps_decay=...,\n",
    "    batch_size=...,\n",
    "    learning_rate=...,\n",
    "    discount_factor=...,\n",
    "    double=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = train(\n",
    "    agent,\n",
    "    env,\n",
    "    log_dir=log_root / \"lunarlander-v2\",\n",
    "    prefix=\"_double_dqn\",\n",
    "    buffer_size=...,\n",
    "    n_steps=2*10**5,\n",
    "    warmup_steps=...,\n",
    "    target_update_every=...,\n",
    "    log_every=4000,\n",
    "    save_every=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You should obtain better curve for Double DQN, however it is not always the case. For such environent the improvement could be neglectable. Although, you have to at least reach the same performance. \n",
    "### You may want to compare the performance on _CartPole_ as, unlike _LunarLander_ , its reward function is dense. However, it may be too simple to reveal any difference.\n",
    "### Thus, it is much better to compare DQN and Double DQN on Atari benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=300)\n",
    "df = pd.read_csv(str(log_root / \"lunarlander-v2\" / \"logs_dqn.csv\"), sep=\";\")\n",
    "df_double = pd.read_csv(str(log_root / \"lunarlander-v2\" / \"logs_double_dqn.csv\"), sep=\";\")\n",
    "plot_progress(df, \"DQN\")\n",
    "plot_progress(df_double, \"Double DQN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the graphs of estimated Q values by DQN and Double DQN. Could you describe what's happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=300)\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.ylabel(\"estimated Q value\")\n",
    "plt.plot(df['misc/timestep'], df['misc/q_est'], label=\"DQN\")\n",
    "plt.plot(df_double['misc/timestep'], df_double['misc/q_est'], label=\"Double DQN\")\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATARI [Optional]\n",
    "\n",
    "In this section, you can try the algorithm on the larger problem from Atari suit. This section is optional. The training procedure is computationally cumbersome, so that ideally one may want to use GPU and powerful CPU for this task. Also note, that usually people choose larger `buffer_size` for this benchmark, which may consume anormous amount of RAM. To avoid the memory overflow, you may consider to use lossless compression algorithm to save a lot of space by compressing observations. See RLLib or Catalyst.RL for example.\n",
    "\n",
    "If you decided to go through the task, note, that typical choice of hyperparameters for Atari differs from previous ones. See RLLib's configs, for example, as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'gym[atari]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clone and install `baselines` lib. We'll use atari wrappers from it.\n",
    "\n",
    "Here are commands for that:\n",
    "```bash\n",
    "git clone https://github.com/openai/baselines\n",
    "cd baselines\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
    "from lib.utils import TransposeImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(name, seed=None, monitor=False):\n",
    "    env = gym.make(name)\n",
    "\n",
    "    is_atari = hasattr(gym.envs, 'atari') and isinstance(\n",
    "        env.unwrapped, gym.envs.atari.atari_env.AtariEnv)\n",
    "\n",
    "    if is_atari:\n",
    "        max_epi_steps = 108000\n",
    "        if max_epi_steps < 0:\n",
    "            max_epi_steps = None\n",
    "\n",
    "        env = make_atari(name, max_epi_steps)\n",
    "        env = wrap_deepmind(env, True, True, True, True)\n",
    "        env = TransposeImage(env, op=[2, 0, 1])\n",
    "        \n",
    "    if monitor:\n",
    "        env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "    if seed:\n",
    "        env.seed(seed)\n",
    "\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"BreakoutNoFrameskip-v4\", monitor=True)\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    act = env.action_space.sample()\n",
    "    if env.step(act)[2]: break\n",
    "env.close()\n",
    "\n",
    "HTML(data=get_html_video_string(env.file_infix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"BreakoutNoFrameskip-v4\", SEED)\n",
    "\n",
    "value_network = get_network(\"vision\")(\n",
    "    env.observation_space.shape[0],\n",
    "    256,\n",
    "    env.action_space.n)\n",
    "\n",
    "agent = DQN(\n",
    "    value_network,\n",
    "    env.action_space,\n",
    "    eps=...,\n",
    "    eps_decay=...,\n",
    "    batch_size=...,\n",
    "    learning_rate=...,\n",
    "    discount_factor=...,\n",
    "    double=...,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = train(\n",
    "    agent,\n",
    "    env,\n",
    "    log_dir=log_root / \"breakout-v4\",\n",
    "    prefix=\"_dqn\",\n",
    "    buffer_size=...,\n",
    "    n_steps=...,\n",
    "    warmup_steps=...,\n",
    "    target_update_every=...,\n",
    "    log_every=...,\n",
    "    save_every=...,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"BreakoutNoFrameskip-v4\", monitor=True)\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0)\n",
    "    act = agent.pick_action(obs, force_greedy=True)\n",
    "    obs, rew, done, info = env.step(act)\n",
    "    if done: break\n",
    "env.close()\n",
    "\n",
    "HTML(data=get_html_video_string(env.file_infix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=300)\n",
    "df = pd.read_csv(str(log_root / \"breakout-v4\" / \"logs_dqn.csv\"), sep=\";\")\n",
    "plot_progress(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
