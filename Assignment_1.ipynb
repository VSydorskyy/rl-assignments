{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import io\n",
    "import base64\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from lib import get_network\n",
    "from lib.agent import DQN\n",
    "from lib.logging import Logger\n",
    "from lib.rollout import ReplayBuffer\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this assignment you are going to:\n",
    "* implement DQN and doubleDQN\n",
    "* use them on CartPole, LunarLander and (Optionally) BreakOut environments\n",
    "\n",
    "### We use PyTorch for neural networks. If you are new to PyTorch see tutorials:\n",
    "https://pytorch.org/tutorials/ <br>\n",
    "https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a closer look at the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(name, seed=None, monitor=False):\n",
    "    env = gym.make(name)\n",
    "    if monitor:\n",
    "        env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "    if seed:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_video_string(infix):\n",
    "    video = io.open(\n",
    "        f'./gym-results/openaigym.video.{infix}.video000000.mp4',\n",
    "        'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    dec_str = encoded.decode('ascii')\n",
    "    src_tag = f'<source src=\"data:video/mp4;base64,{dec_str}\" type=\"video/mp4\" /></video>'\n",
    "    html = f'<video width=\"360\" height=\"auto\" alt=\"test\" controls>{src_tag}</video>'\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0xCEEEEEEB\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_root = Path(\"results\")\n",
    "log_root.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will embed a video of random policy playing CartPole-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" /></video></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = make_env(\"CartPole-v1\", monitor=True)\n",
    "env.reset()\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    if env.step(action)[2]: break\n",
    "env.close()\n",
    "\n",
    "HTML(data=get_html_video_string(env.file_infix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "Here's the definition of a function that implements training loop. We'll get through its elements soon.\n",
    "Although, note that we use a warmup techinque to fill the buffer in the beginning of a training procedure.\n",
    "\n",
    "The loop consists of:\n",
    "* Picking an action using the policy:\n",
    "$$a = \\text{argmax}_a Q(s, a)$$\n",
    "* Sending the action to environment (`step`)\n",
    "* Sampling N examples from the ReplayBuffer:\n",
    "$$ (s_i, a_i, s^\\prime_i, r_i)_{i=1,..,N} $$\n",
    "* Updating Q-network by minimizing the loss:\n",
    "$$y = r_i + \\gamma \\text{max}_a Q_{target}(s^{\\prime}_i, a)$$\n",
    "\n",
    "$$L_i = \\text{Loss}(Q(s_i, a_i), y)$$\n",
    "$$L = \\frac{1}{N} \\sum_i L_i \\rightarrow min,$$\n",
    "where $\\text{Loss}$ is $L_2$ loss or Huber loss\n",
    "* Updating target network every `target_update_every` iteration:\n",
    "$$\\theta^\\prime \\leftarrow \\theta \\tau + \\theta^\\prime (1 - \\tau)$$\n",
    "where $\\theta^\\prime$ - paremeters of $Q_{target}$ network, <br>\n",
    "$\\theta$ - paremeters of $Q$ network,<br>\n",
    "$\\tau \\in [0, 1]$\n",
    "\n",
    "Note, that we also use a logger that writes training info to `CSV` file, `TensorBoard` and `stdout`. We use `CSV` and `stdout` throughout the notebook, but you can use TB for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 0: Implement ReplayBuffer from `lib/rollout.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    agent,\n",
    "    env,\n",
    "    *,\n",
    "    log_dir,\n",
    "    prefix,\n",
    "    buffer_size,\n",
    "    n_steps,\n",
    "    warmup_steps,\n",
    "    target_update_every,\n",
    "    log_every,\n",
    "    save_every\n",
    "):\n",
    "    \"\"\"Training loop procedure.\n",
    "    \n",
    "    Arguments:\n",
    "        agent: DQN object.\n",
    "        env: Gym environment.\n",
    "        log_dir: PathLib path to logging dir.\n",
    "        prefix: Prefix for the experiment.\n",
    "        buffer_size: Maximum capacity of the replay buffer.\n",
    "        n_steps: Total number of training iterations.\n",
    "        warmup_steps: Number of first iterations that employ random policy.\n",
    "        target_update_every: Number of iterations between target network updates.\n",
    "        log_every: Log frequency\n",
    "        save_every: Save frequency\n",
    "        \n",
    "    Returns agent after training.\n",
    "    \"\"\"\n",
    "    \n",
    "    logger = Logger(log_dir, prefix)\n",
    "    episode_reward = []\n",
    "    memory = ReplayBuffer(buffer_size)\n",
    "    \n",
    "    obs_cur = env.reset()\n",
    "    for i in range(n_steps + 1):\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "        if i > warmup_steps:\n",
    "            obs = torch.FloatTensor(obs_cur).unsqueeze(0)\n",
    "            act = agent.pick_action(obs)\n",
    "        else:\n",
    "            act = env.action_space.sample()\n",
    "\n",
    "        obs_prev = obs_cur\n",
    "        obs_cur, rew, done, info = env.step(act)\n",
    "        episode_reward.append(rew)\n",
    "\n",
    "        memory.push(obs_prev, act, obs_cur, rew, done)\n",
    "\n",
    "        if done:\n",
    "            rew = np.sum(episode_reward)\n",
    "            episode_reward = []\n",
    "            logger.log_arr_kv(\"reward/reward\", rew)\n",
    "            obs_cur = env.reset()\n",
    "\n",
    "        if i > warmup_steps:\n",
    "\n",
    "            loss, q, q_est = agent.update_value(memory)\n",
    "            logger.log_arr_kv(\"loss/bellman_error_t\", loss)\n",
    "            logger.log_arr_kv(\"misc/q_t\", q)\n",
    "            logger.log_arr_kv(\"misc/q_est_t\", q_est)\n",
    "\n",
    "            if i % target_update_every == 0:\n",
    "                agent.update_target()\n",
    "\n",
    "            if i % log_every == 0:\n",
    "                logger.reduce_arr_kv(\"reward/reward\", \"reward/reward_mean\", np.mean)\n",
    "                logger.reduce_arr_kv(\"reward/reward\", \"reward/reward_max\", np.max)\n",
    "                logger.reduce_arr_kv(\"reward/reward\", \"reward/reward_min\", np.min)\n",
    "                logger.reduce_arr_kv(\"reward/reward\", \"reward/reward_std\", np.std)\n",
    "                logger.reduce_arr_kv(\"loss/bellman_error_t\", \"loss/bellman_error\", np.mean)\n",
    "                logger.reduce_arr_kv(\"misc/q_t\", \"misc/q\", np.mean)\n",
    "                logger.reduce_arr_kv(\"misc/q_est_t\", \"misc/q_est\", np.mean)\n",
    "                logger.log_kv(\"misc/epsilon\", agent.eps)\n",
    "                logger.log_kv(\"misc/timestep\", i)\n",
    "                logger.write_logs(skip_arrs=True)\n",
    "\n",
    "            if i % save_every == 0:\n",
    "                num = i // save_every + 1\n",
    "                save_dir = log_dir / \"checkpoints\"\n",
    "                save_dir.mkdir(exist_ok=True)\n",
    "                agent.save_to(save_dir, prefix=hex(num)[2:].upper())\n",
    "\n",
    "        agent.update_eps()\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "#### Task 1: Implement MLP network in `lib/network.py`\n",
    "#### Task 2: Implement DQN in `lib/agent.py`\n",
    "\n",
    "Now, run the next two cells. You should obtain the highest (500) reward in less than 100000 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note! You can try to use small `target_update_every`. E.g. value of 1 corresponds to fitted Q-iteration. The expected outcome is unstable or even divergent behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"CartPole-v1\", SEED)\n",
    "\n",
    "value_network = get_network(\"mlp\")(\n",
    "    env.observation_space.shape[0],\n",
    "    64,\n",
    "    env.action_space.n)\n",
    "\n",
    "agent = DQN(\n",
    "    value_network,\n",
    "    env.action_space,\n",
    "    eps=.2,\n",
    "    eps_decay=(.2 - .02) / 10**5,\n",
    "    batch_size=32,\n",
    "    learning_rate=5e-4,\n",
    "    discount_factor=.99\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| reward/reward_mean | 17.326   |\n",
      "| reward/reward_max  | 46       |\n",
      "| reward/reward_min  | 8        |\n",
      "| reward/reward_std  | 9.0842   |\n",
      "| loss/bellman_error | 0.077766 |\n",
      "| misc/q             | 1.184    |\n",
      "| misc/q_est         | 0.96439  |\n",
      "| misc/epsilon       | 0.1973   |\n",
      "| misc/timestep      | 1500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 12.575   |\n",
      "| reward/reward_max  | 18       |\n",
      "| reward/reward_min  | 9        |\n",
      "| reward/reward_std  | 2.0723   |\n",
      "| loss/bellman_error | 0.054855 |\n",
      "| misc/q             | 2.1102   |\n",
      "| misc/q_est         | 2.0513   |\n",
      "| misc/epsilon       | 0.1964   |\n",
      "| misc/timestep      | 2000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 14.111   |\n",
      "| reward/reward_max  | 29       |\n",
      "| reward/reward_min  | 8        |\n",
      "| reward/reward_std  | 4.9373   |\n",
      "| loss/bellman_error | 0.092835 |\n",
      "| misc/q             | 2.9595   |\n",
      "| misc/q_est         | 2.9678   |\n",
      "| misc/epsilon       | 0.1955   |\n",
      "| misc/timestep      | 2500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 19.68    |\n",
      "| reward/reward_max  | 36       |\n",
      "| reward/reward_min  | 9        |\n",
      "| reward/reward_std  | 5.6619   |\n",
      "| loss/bellman_error | 0.13768  |\n",
      "| misc/q             | 3.8181   |\n",
      "| misc/q_est         | 3.8858   |\n",
      "| misc/epsilon       | 0.1946   |\n",
      "| misc/timestep      | 3000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 20.625   |\n",
      "| reward/reward_max  | 43       |\n",
      "| reward/reward_min  | 11       |\n",
      "| reward/reward_std  | 7.387    |\n",
      "| loss/bellman_error | 0.18089  |\n",
      "| misc/q             | 4.6676   |\n",
      "| misc/q_est         | 4.7835   |\n",
      "| misc/epsilon       | 0.1937   |\n",
      "| misc/timestep      | 3500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 26.833   |\n",
      "| reward/reward_max  | 46       |\n",
      "| reward/reward_min  | 16       |\n",
      "| reward/reward_std  | 8.7385   |\n",
      "| loss/bellman_error | 0.21374  |\n",
      "| misc/q             | 5.4877   |\n",
      "| misc/q_est         | 5.64     |\n",
      "| misc/epsilon       | 0.1928   |\n",
      "| misc/timestep      | 4000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 36.857   |\n",
      "| reward/reward_max  | 71       |\n",
      "| reward/reward_min  | 18       |\n",
      "| reward/reward_std  | 15.643   |\n",
      "| loss/bellman_error | 0.23211  |\n",
      "| misc/q             | 6.3373   |\n",
      "| misc/q_est         | 6.5133   |\n",
      "| misc/epsilon       | 0.1919   |\n",
      "| misc/timestep      | 4500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 41.75    |\n",
      "| reward/reward_max  | 74       |\n",
      "| reward/reward_min  | 25       |\n",
      "| reward/reward_std  | 12.282   |\n",
      "| loss/bellman_error | 0.2507   |\n",
      "| misc/q             | 7.2059   |\n",
      "| misc/q_est         | 7.4043   |\n",
      "| misc/epsilon       | 0.191    |\n",
      "| misc/timestep      | 5000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 50.2     |\n",
      "| reward/reward_max  | 70       |\n",
      "| reward/reward_min  | 37       |\n",
      "| reward/reward_std  | 12.335   |\n",
      "| loss/bellman_error | 0.26254  |\n",
      "| misc/q             | 8.057    |\n",
      "| misc/q_est         | 8.2723   |\n",
      "| misc/epsilon       | 0.1901   |\n",
      "| misc/timestep      | 5500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 66.714   |\n",
      "| reward/reward_max  | 108      |\n",
      "| reward/reward_min  | 43       |\n",
      "| reward/reward_std  | 19.181   |\n",
      "| loss/bellman_error | 0.29016  |\n",
      "| misc/q             | 8.8914   |\n",
      "| misc/q_est         | 9.135    |\n",
      "| misc/epsilon       | 0.1892   |\n",
      "| misc/timestep      | 6000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 84       |\n",
      "| reward/reward_max  | 151      |\n",
      "| reward/reward_min  | 48       |\n",
      "| reward/reward_std  | 35.935   |\n",
      "| loss/bellman_error | 0.31059  |\n",
      "| misc/q             | 9.7389   |\n",
      "| misc/q_est         | 10.005   |\n",
      "| misc/epsilon       | 0.1883   |\n",
      "| misc/timestep      | 6500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 99.25    |\n",
      "| reward/reward_max  | 125      |\n",
      "| reward/reward_min  | 75       |\n",
      "| reward/reward_std  | 19.778   |\n",
      "| loss/bellman_error | 0.31216  |\n",
      "| misc/q             | 10.608   |\n",
      "| misc/q_est         | 10.879   |\n",
      "| misc/epsilon       | 0.1874   |\n",
      "| misc/timestep      | 7000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 79.875   |\n",
      "| reward/reward_max  | 158      |\n",
      "| reward/reward_min  | 43       |\n",
      "| reward/reward_std  | 35.855   |\n",
      "| loss/bellman_error | 0.33459  |\n",
      "| misc/q             | 11.523   |\n",
      "| misc/q_est         | 11.818   |\n",
      "| misc/epsilon       | 0.1865   |\n",
      "| misc/timestep      | 7500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 157      |\n",
      "| reward/reward_max  | 265      |\n",
      "| reward/reward_min  | 93       |\n",
      "| reward/reward_std  | 76.803   |\n",
      "| loss/bellman_error | 0.3312   |\n",
      "| misc/q             | 12.499   |\n",
      "| misc/q_est         | 12.791   |\n",
      "| misc/epsilon       | 0.1856   |\n",
      "| misc/timestep      | 8000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 88.5     |\n",
      "| reward/reward_max  | 153      |\n",
      "| reward/reward_min  | 41       |\n",
      "| reward/reward_std  | 35.269   |\n",
      "| loss/bellman_error | 0.37426  |\n",
      "| misc/q             | 13.357   |\n",
      "| misc/q_est         | 13.693   |\n",
      "| misc/epsilon       | 0.1847   |\n",
      "| misc/timestep      | 8500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 68.333   |\n",
      "| reward/reward_max  | 97       |\n",
      "| reward/reward_min  | 45       |\n",
      "| reward/reward_std  | 16.449   |\n",
      "| loss/bellman_error | 0.37357  |\n",
      "| misc/q             | 14.299   |\n",
      "| misc/q_est         | 14.637   |\n",
      "| misc/epsilon       | 0.1838   |\n",
      "| misc/timestep      | 9000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 106      |\n",
      "| reward/reward_max  | 185      |\n",
      "| reward/reward_min  | 52       |\n",
      "| reward/reward_std  | 44.842   |\n",
      "| loss/bellman_error | 0.36219  |\n",
      "| misc/q             | 15.233   |\n",
      "| misc/q_est         | 15.561   |\n",
      "| misc/epsilon       | 0.1829   |\n",
      "| misc/timestep      | 9500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 78.143   |\n",
      "| reward/reward_max  | 128      |\n",
      "| reward/reward_min  | 51       |\n",
      "| reward/reward_std  | 22.319   |\n",
      "| loss/bellman_error | 0.4204   |\n",
      "| misc/q             | 16.079   |\n",
      "| misc/q_est         | 16.464   |\n",
      "| misc/epsilon       | 0.182    |\n",
      "| misc/timestep      | 10000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 42.636   |\n",
      "| reward/reward_max  | 66       |\n",
      "| reward/reward_min  | 28       |\n",
      "| reward/reward_std  | 11.734   |\n",
      "| loss/bellman_error | 0.41208  |\n",
      "| misc/q             | 16.935   |\n",
      "| misc/q_est         | 17.313   |\n",
      "| misc/epsilon       | 0.1811   |\n",
      "| misc/timestep      | 10500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 52.7     |\n",
      "| reward/reward_max  | 156      |\n",
      "| reward/reward_min  | 26       |\n",
      "| reward/reward_std  | 36.164   |\n",
      "| loss/bellman_error | 0.4043   |\n",
      "| misc/q             | 17.862   |\n",
      "| misc/q_est         | 18.236   |\n",
      "| misc/epsilon       | 0.1802   |\n",
      "| misc/timestep      | 11000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 33.867   |\n",
      "| reward/reward_max  | 49       |\n",
      "| reward/reward_min  | 17       |\n",
      "| reward/reward_std  | 8.0983   |\n",
      "| loss/bellman_error | 0.43493  |\n",
      "| misc/q             | 18.728   |\n",
      "| misc/q_est         | 19.133   |\n",
      "| misc/epsilon       | 0.1793   |\n",
      "| misc/timestep      | 11500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 22.545   |\n",
      "| reward/reward_max  | 41       |\n",
      "| reward/reward_min  | 8        |\n",
      "| reward/reward_std  | 9.2083   |\n",
      "| loss/bellman_error | 0.47841  |\n",
      "| misc/q             | 19.555   |\n",
      "| misc/q_est         | 20.002   |\n",
      "| misc/epsilon       | 0.1784   |\n",
      "| misc/timestep      | 12000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 33.667   |\n",
      "| reward/reward_max  | 50       |\n",
      "| reward/reward_min  | 22       |\n",
      "| reward/reward_std  | 8.592    |\n",
      "| loss/bellman_error | 0.4963   |\n",
      "| misc/q             | 20.464   |\n",
      "| misc/q_est         | 20.929   |\n",
      "| misc/epsilon       | 0.1775   |\n",
      "| misc/timestep      | 12500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 24.4     |\n",
      "| reward/reward_max  | 46       |\n",
      "| reward/reward_min  | 12       |\n",
      "| reward/reward_std  | 7.3171   |\n",
      "| loss/bellman_error | 0.5367   |\n",
      "| misc/q             | 21.342   |\n",
      "| misc/q_est         | 21.848   |\n",
      "| misc/epsilon       | 0.1766   |\n",
      "| misc/timestep      | 13000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 22.636   |\n",
      "| reward/reward_max  | 40       |\n",
      "| reward/reward_min  | 12       |\n",
      "| reward/reward_std  | 6.5889   |\n",
      "| loss/bellman_error | 0.54256  |\n",
      "| misc/q             | 22.182   |\n",
      "| misc/q_est         | 22.695   |\n",
      "| misc/epsilon       | 0.1757   |\n",
      "| misc/timestep      | 13500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 18.926   |\n",
      "| reward/reward_max  | 31       |\n",
      "| reward/reward_min  | 9        |\n",
      "| reward/reward_std  | 5.702    |\n",
      "| loss/bellman_error | 0.64688  |\n",
      "| misc/q             | 22.927   |\n",
      "| misc/q_est         | 23.542   |\n",
      "| misc/epsilon       | 0.1748   |\n",
      "| misc/timestep      | 14000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 18.036   |\n",
      "| reward/reward_max  | 31       |\n",
      "| reward/reward_min  | 9        |\n",
      "| reward/reward_std  | 6.3273   |\n",
      "| loss/bellman_error | 0.64804  |\n",
      "| misc/q             | 23.805   |\n",
      "| misc/q_est         | 24.422   |\n",
      "| misc/epsilon       | 0.1739   |\n",
      "| misc/timestep      | 14500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 21.043   |\n",
      "| reward/reward_max  | 46       |\n",
      "| reward/reward_min  | 9        |\n",
      "| reward/reward_std  | 11.277   |\n",
      "| loss/bellman_error | 0.68806  |\n",
      "| misc/q             | 24.649   |\n",
      "| misc/q_est         | 25.306   |\n",
      "| misc/epsilon       | 0.173    |\n",
      "| misc/timestep      | 15000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 55       |\n",
      "| reward/reward_max  | 122      |\n",
      "| reward/reward_min  | 25       |\n",
      "| reward/reward_std  | 26.115   |\n",
      "| loss/bellman_error | 0.76678  |\n",
      "| misc/q             | 25.374   |\n",
      "| misc/q_est         | 26.108   |\n",
      "| misc/epsilon       | 0.1721   |\n",
      "| misc/timestep      | 15500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 92.6     |\n",
      "| reward/reward_max  | 109      |\n",
      "| reward/reward_min  | 85       |\n",
      "| reward/reward_std  | 8.4522   |\n",
      "| loss/bellman_error | 0.70676  |\n",
      "| misc/q             | 26.24    |\n",
      "| misc/q_est         | 26.916   |\n",
      "| misc/epsilon       | 0.1712   |\n",
      "| misc/timestep      | 16000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 76       |\n",
      "| reward/reward_max  | 94       |\n",
      "| reward/reward_min  | 54       |\n",
      "| reward/reward_std  | 13.47    |\n",
      "| loss/bellman_error | 0.80065  |\n",
      "| misc/q             | 26.936   |\n",
      "| misc/q_est         | 27.705   |\n",
      "| misc/epsilon       | 0.1703   |\n",
      "| misc/timestep      | 16500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 67.571   |\n",
      "| reward/reward_max  | 81       |\n",
      "| reward/reward_min  | 54       |\n",
      "| reward/reward_std  | 9.5148   |\n",
      "| loss/bellman_error | 0.80365  |\n",
      "| misc/q             | 27.632   |\n",
      "| misc/q_est         | 28.404   |\n",
      "| misc/epsilon       | 0.1694   |\n",
      "| misc/timestep      | 17000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 119.5    |\n",
      "| reward/reward_max  | 166      |\n",
      "| reward/reward_min  | 96       |\n",
      "| reward/reward_std  | 27.518   |\n",
      "| loss/bellman_error | 0.79397  |\n",
      "| misc/q             | 28.438   |\n",
      "| misc/q_est         | 29.201   |\n",
      "| misc/epsilon       | 0.1685   |\n",
      "| misc/timestep      | 17500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 121.75   |\n",
      "| reward/reward_max  | 137      |\n",
      "| reward/reward_min  | 99       |\n",
      "| reward/reward_std  | 14.923   |\n",
      "| loss/bellman_error | 0.75184  |\n",
      "| misc/q             | 29.25    |\n",
      "| misc/q_est         | 29.973   |\n",
      "| misc/epsilon       | 0.1676   |\n",
      "| misc/timestep      | 18000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 144.75   |\n",
      "| reward/reward_max  | 200      |\n",
      "| reward/reward_min  | 123      |\n",
      "| reward/reward_std  | 32.089   |\n",
      "| loss/bellman_error | 0.72011  |\n",
      "| misc/q             | 30.03    |\n",
      "| misc/q_est         | 30.722   |\n",
      "| misc/epsilon       | 0.1667   |\n",
      "| misc/timestep      | 18500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 145.67   |\n",
      "| reward/reward_max  | 180      |\n",
      "| reward/reward_min  | 125      |\n",
      "| reward/reward_std  | 24.445   |\n",
      "| loss/bellman_error | 0.88952  |\n",
      "| misc/q             | 30.618   |\n",
      "| misc/q_est         | 31.476   |\n",
      "| misc/epsilon       | 0.1658   |\n",
      "| misc/timestep      | 19000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 160.67   |\n",
      "| reward/reward_max  | 189      |\n",
      "| reward/reward_min  | 118      |\n",
      "| reward/reward_std  | 30.706   |\n",
      "| loss/bellman_error | 0.79823  |\n",
      "| misc/q             | 31.466   |\n",
      "| misc/q_est         | 32.235   |\n",
      "| misc/epsilon       | 0.1649   |\n",
      "| misc/timestep      | 19500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 192.67   |\n",
      "| reward/reward_max  | 283      |\n",
      "| reward/reward_min  | 130      |\n",
      "| reward/reward_std  | 65.454   |\n",
      "| loss/bellman_error | 0.77713  |\n",
      "| misc/q             | 32.227   |\n",
      "| misc/q_est         | 32.976   |\n",
      "| misc/epsilon       | 0.164    |\n",
      "| misc/timestep      | 20000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 142.33   |\n",
      "| reward/reward_max  | 187      |\n",
      "| reward/reward_min  | 117      |\n",
      "| reward/reward_std  | 31.679   |\n",
      "| loss/bellman_error | 0.75637  |\n",
      "| misc/q             | 33       |\n",
      "| misc/q_est         | 33.729   |\n",
      "| misc/epsilon       | 0.1631   |\n",
      "| misc/timestep      | 20500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 130.5    |\n",
      "| reward/reward_max  | 190      |\n",
      "| reward/reward_min  | 99       |\n",
      "| reward/reward_std  | 35.103   |\n",
      "| loss/bellman_error | 0.74204  |\n",
      "| misc/q             | 33.762   |\n",
      "| misc/q_est         | 34.478   |\n",
      "| misc/epsilon       | 0.1622   |\n",
      "| misc/timestep      | 21000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 140      |\n",
      "| reward/reward_max  | 195      |\n",
      "| reward/reward_min  | 110      |\n",
      "| reward/reward_std  | 32.55    |\n",
      "| loss/bellman_error | 0.83076  |\n",
      "| misc/q             | 34.39    |\n",
      "| misc/q_est         | 35.193   |\n",
      "| misc/epsilon       | 0.1613   |\n",
      "| misc/timestep      | 21500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 110.25   |\n",
      "| reward/reward_max  | 120      |\n",
      "| reward/reward_min  | 90       |\n",
      "| reward/reward_std  | 12.091   |\n",
      "| loss/bellman_error | 0.78204  |\n",
      "| misc/q             | 35.199   |\n",
      "| misc/q_est         | 35.954   |\n",
      "| misc/epsilon       | 0.1604   |\n",
      "| misc/timestep      | 22000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 120.75   |\n",
      "| reward/reward_max  | 135      |\n",
      "| reward/reward_min  | 108      |\n",
      "| reward/reward_std  | 9.6792   |\n",
      "| loss/bellman_error | 0.80678  |\n",
      "| misc/q             | 35.861   |\n",
      "| misc/q_est         | 36.641   |\n",
      "| misc/epsilon       | 0.1595   |\n",
      "| misc/timestep      | 22500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 157.67   |\n",
      "| reward/reward_max  | 241      |\n",
      "| reward/reward_min  | 116      |\n",
      "| reward/reward_std  | 58.926   |\n",
      "| loss/bellman_error | 0.85783  |\n",
      "| misc/q             | 36.558   |\n",
      "| misc/q_est         | 37.388   |\n",
      "| misc/epsilon       | 0.1586   |\n",
      "| misc/timestep      | 23000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 106      |\n",
      "| reward/reward_max  | 120      |\n",
      "| reward/reward_min  | 88       |\n",
      "| reward/reward_std  | 11.261   |\n",
      "| loss/bellman_error | 0.8396   |\n",
      "| misc/q             | 37.29    |\n",
      "| misc/q_est         | 38.102   |\n",
      "| misc/epsilon       | 0.1577   |\n",
      "| misc/timestep      | 23500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 132.5    |\n",
      "| reward/reward_max  | 139      |\n",
      "| reward/reward_min  | 126      |\n",
      "| reward/reward_std  | 4.717    |\n",
      "| loss/bellman_error | 0.89941  |\n",
      "| misc/q             | 37.947   |\n",
      "| misc/q_est         | 38.819   |\n",
      "| misc/epsilon       | 0.1568   |\n",
      "| misc/timestep      | 24000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 153.33   |\n",
      "| reward/reward_max  | 195      |\n",
      "| reward/reward_min  | 120      |\n",
      "| reward/reward_std  | 31.18    |\n",
      "| loss/bellman_error | 0.91568  |\n",
      "| misc/q             | 38.604   |\n",
      "| misc/q_est         | 39.492   |\n",
      "| misc/epsilon       | 0.1559   |\n",
      "| misc/timestep      | 24500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 132.25   |\n",
      "| reward/reward_max  | 160      |\n",
      "| reward/reward_min  | 114      |\n",
      "| reward/reward_std  | 17.384   |\n",
      "| loss/bellman_error | 0.89183  |\n",
      "| misc/q             | 39.302   |\n",
      "| misc/q_est         | 40.168   |\n",
      "| misc/epsilon       | 0.155    |\n",
      "| misc/timestep      | 25000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 121.75   |\n",
      "| reward/reward_max  | 140      |\n",
      "| reward/reward_min  | 110      |\n",
      "| reward/reward_std  | 12.296   |\n",
      "| loss/bellman_error | 0.95386  |\n",
      "| misc/q             | 39.864   |\n",
      "| misc/q_est         | 40.789   |\n",
      "| misc/epsilon       | 0.1541   |\n",
      "| misc/timestep      | 25500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 113      |\n",
      "| reward/reward_max  | 123      |\n",
      "| reward/reward_min  | 101      |\n",
      "| reward/reward_std  | 8.9163   |\n",
      "| loss/bellman_error | 0.86015  |\n",
      "| misc/q             | 40.652   |\n",
      "| misc/q_est         | 41.487   |\n",
      "| misc/epsilon       | 0.1532   |\n",
      "| misc/timestep      | 26000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| reward/reward_mean | 123.2    |\n",
      "| reward/reward_max  | 152      |\n",
      "| reward/reward_min  | 100      |\n",
      "| reward/reward_std  | 17.429   |\n",
      "| loss/bellman_error | 0.8992   |\n",
      "| misc/q             | 41.259   |\n",
      "| misc/q_est         | 42.132   |\n",
      "| misc/epsilon       | 0.1523   |\n",
      "| misc/timestep      | 26500    |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "agent = train(\n",
    "    agent,\n",
    "    env,\n",
    "    log_dir=log_root / \"cartpole-v1\",\n",
    "    prefix=\"_dqn\",\n",
    "    buffer_size=5 * 10**4,\n",
    "    n_steps=10**5,\n",
    "    warmup_steps=1000,\n",
    "    target_update_every=500,\n",
    "    log_every=500,\n",
    "    save_every=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at our learned policy in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"CartPole-v1\", monitor=True)\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0)\n",
    "    act = agent.pick_action(obs, force_greedy=True)\n",
    "    obs, rew, done, info = env.step(act)\n",
    "    if done: break\n",
    "env.close()\n",
    "\n",
    "HTML(data=get_html_video_string(env.file_infix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_progress(data, name=None):\n",
    "    name = name or \"Mean reward\"\n",
    "    plt.plot(data['misc/timestep'], data['reward/reward_mean'], label=name)\n",
    "    lower = np.array(data['reward/reward_mean'] - data['reward/reward_std']).clip(-1e5, +1e5)\n",
    "    upper = np.array(data['reward/reward_mean'] + data['reward/reward_std']).clip(-1e5, +1e5)\n",
    "    examples = np.array(data['misc/timestep'])\n",
    "    plt.fill_between(list(examples), list(lower), list(upper), color='blue', alpha=0.1)\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel(\"Timestep\")\n",
    "    plt.ylabel(\"Reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell plots a mean reward against training iterations. Note, that the agent may sometimes degrade its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=300)\n",
    "df = pd.read_csv(str(log_root / \"cartpole-v1\" / \"logs_dqn.csv\"), sep=\";\")\n",
    "plot_progress(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LunarLander\n",
    "\n",
    "Okay, now that we're done with very simple CartPole environment, let's move to something more complicated. \n",
    "\n",
    "LunarLander is a simulation game where a player has to control the capsule and land it on the zone marked with flags. If an agent achieves more than 200 scores, the environment is said to be solved.\n",
    "\n",
    "##### First, you should install Box2D support in Gym: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'gym[box2d]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The random policy quickly fails the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"LunarLander-v2\", monitor=True)\n",
    "env.reset()\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    if env.step(action)[2]: break\n",
    "env.close()\n",
    "\n",
    "HTML(data=get_html_video_string(env.file_infix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Play a bit with hyperparameters. You have to gain some intuition about what each of them does. You have to achieve mean score of 200 in ~200 000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"LunarLander-v2\", SEED)\n",
    "\n",
    "value_network = get_network(\"mlp\")(\n",
    "    env.observation_space.shape[0],\n",
    "    ...,\n",
    "    env.action_space.n)\n",
    "\n",
    "agent = DQN(\n",
    "    value_network,\n",
    "    env.action_space,\n",
    "    eps=...,\n",
    "    eps_decay=...,\n",
    "    batch_size=...,\n",
    "    learning_rate=...,\n",
    "    discount_factor=...,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = train(\n",
    "    agent,\n",
    "    env,\n",
    "    log_dir=log_root / \"lunarlander-v2\",\n",
    "    prefix=\"_dqn\",\n",
    "    buffer_size=...,\n",
    "    n_steps=2*10**5,\n",
    "    warmup_steps=...,\n",
    "    target_update_every=...,\n",
    "    log_every=4000,\n",
    "    save_every=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"LunarLander-v2\", monitor=True)\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0)\n",
    "    act = agent.pick_action(obs, force_greedy=True)\n",
    "    obs, rew, done, info = env.step(act)\n",
    "    if done: break\n",
    "env.close()\n",
    "\n",
    "HTML(data=get_html_video_string(env.file_infix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=300)\n",
    "df = pd.read_csv(str(log_root / \"lunarlander-v2\" / \"logs_dqn.csv\"), sep=\";\")\n",
    "plot_progress(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main drawbacks of vanila DQN is that it overestimates Q-values. More details could be found in the original paper. In a couple of words, we have to estimate an expecation of maximum, not a maximum of expectations as we do in vanila DQN. The trick is to use the second network for the updates (we use target network for this):\n",
    "\n",
    "$$y = r + \\gamma Q_{target}(s^\\prime, \\text{argmax}_a Q(s^\\prime, a) )$$\n",
    "\n",
    "Compared to \n",
    "\n",
    "$$y = r + \\gamma Q_{target}(s^\\prime, \\text{argmax}_a Q_{target}(s^\\prime, a) )$$\n",
    "\n",
    "from vanila DQN. Which is the same as:\n",
    "\n",
    "$$y = r + \\gamma \\text{max}_a Q_{target}(s^\\prime, a)$$\n",
    "\n",
    "Double Q-learning paper: https://papers.nips.cc/paper/3964-double-q-learning <br>\n",
    "Double DQN paper: https://arxiv.org/abs/1509.06461\n",
    "\n",
    "#### Task 4. Implement Double DQN in `lib/agent.py`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, let's try it for the LunarLander environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"LunarLander-v2\", SEED)\n",
    "\n",
    "value_network = get_network(\"mlp\")(\n",
    "    env.observation_space.shape[0],\n",
    "    ...,\n",
    "    env.action_space.n)\n",
    "\n",
    "agent = DQN(\n",
    "    value_network,\n",
    "    env.action_space,\n",
    "    eps=...,\n",
    "    eps_decay=...,\n",
    "    batch_size=...,\n",
    "    learning_rate=...,\n",
    "    discount_factor=...,\n",
    "    double=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = train(\n",
    "    agent,\n",
    "    env,\n",
    "    log_dir=log_root / \"lunarlander-v2\",\n",
    "    prefix=\"_double_dqn\",\n",
    "    buffer_size=...,\n",
    "    n_steps=2*10**5,\n",
    "    warmup_steps=...,\n",
    "    target_update_every=...,\n",
    "    log_every=4000,\n",
    "    save_every=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You should obtain better curve for Double DQN, however it is not always the case. For such environent the improvement could be neglectable. Although, you have to at least reach the same performance. \n",
    "### You may want to compare the performance on _CartPole_ as, unlike _LunarLander_ , its reward function is dense. However, it may be too simple to reveal any difference.\n",
    "### Thus, it is much better to compare DQN and Double DQN on Atari benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=300)\n",
    "df = pd.read_csv(str(log_root / \"lunarlander-v2\" / \"logs_dqn.csv\"), sep=\";\")\n",
    "df_double = pd.read_csv(str(log_root / \"lunarlander-v2\" / \"logs_double_dqn.csv\"), sep=\";\")\n",
    "plot_progress(df, \"DQN\")\n",
    "plot_progress(df_double, \"Double DQN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the graphs of estimated Q values by DQN and Double DQN. Could you describe what's happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=300)\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.ylabel(\"estimated Q value\")\n",
    "plt.plot(df['misc/timestep'], df['misc/q_est'], label=\"DQN\")\n",
    "plt.plot(df_double['misc/timestep'], df_double['misc/q_est'], label=\"Double DQN\")\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATARI [Optional]\n",
    "\n",
    "In this section, you can try the algorithm on the larger problem from Atari suit. This section is optional. The training procedure is computationally cumbersome, so that ideally one may want to use GPU and powerful CPU for this task. Also note, that usually people choose larger `buffer_size` for this benchmark, which may consume anormous amount of RAM. To avoid the memory overflow, you may consider to use lossless compression algorithm to save a lot of space by compressing observations. See RLLib or Catalyst.RL for example.\n",
    "\n",
    "If you decided to go through the task, note, that typical choice of hyperparameters for Atari differs from previous ones. See RLLib's configs, for example, as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'gym[atari]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clone and install `baselines` lib. We'll use atari wrappers from it.\n",
    "\n",
    "Here are commands for that:\n",
    "```bash\n",
    "git clone https://github.com/openai/baselines\n",
    "cd baselines\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
    "from lib.utils import TransposeImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(name, seed=None, monitor=False):\n",
    "    env = gym.make(name)\n",
    "\n",
    "    is_atari = hasattr(gym.envs, 'atari') and isinstance(\n",
    "        env.unwrapped, gym.envs.atari.atari_env.AtariEnv)\n",
    "\n",
    "    if is_atari:\n",
    "        max_epi_steps = 108000\n",
    "        if max_epi_steps < 0:\n",
    "            max_epi_steps = None\n",
    "\n",
    "        env = make_atari(name, max_epi_steps)\n",
    "        env = wrap_deepmind(env, True, True, True, True)\n",
    "        env = TransposeImage(env, op=[2, 0, 1])\n",
    "        \n",
    "    if monitor:\n",
    "        env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "    if seed:\n",
    "        env.seed(seed)\n",
    "\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"BreakoutNoFrameskip-v4\", monitor=True)\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    act = env.action_space.sample()\n",
    "    if env.step(act)[2]: break\n",
    "env.close()\n",
    "\n",
    "HTML(data=get_html_video_string(env.file_infix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"BreakoutNoFrameskip-v4\", SEED)\n",
    "\n",
    "value_network = get_network(\"vision\")(\n",
    "    env.observation_space.shape[0],\n",
    "    256,\n",
    "    env.action_space.n)\n",
    "\n",
    "agent = DQN(\n",
    "    value_network,\n",
    "    env.action_space,\n",
    "    eps=...,\n",
    "    eps_decay=...,\n",
    "    batch_size=...,\n",
    "    learning_rate=...,\n",
    "    discount_factor=...,\n",
    "    double=...,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = train(\n",
    "    agent,\n",
    "    env,\n",
    "    log_dir=log_root / \"breakout-v4\",\n",
    "    prefix=\"_dqn\",\n",
    "    buffer_size=...,\n",
    "    n_steps=...,\n",
    "    warmup_steps=...,\n",
    "    target_update_every=...,\n",
    "    log_every=...,\n",
    "    save_every=...,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"BreakoutNoFrameskip-v4\", monitor=True)\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0)\n",
    "    act = agent.pick_action(obs, force_greedy=True)\n",
    "    obs, rew, done, info = env.step(act)\n",
    "    if done: break\n",
    "env.close()\n",
    "\n",
    "HTML(data=get_html_video_string(env.file_infix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=300)\n",
    "df = pd.read_csv(str(log_root / \"breakout-v4\" / \"logs_dqn.csv\"), sep=\";\")\n",
    "plot_progress(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
